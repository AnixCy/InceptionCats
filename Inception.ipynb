{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using PreTrained InceptionV3, apply transfer learning to classify cat breeds #",
   "id": "71830abf7a18655f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Install ###",
   "id": "45a936ed1bed20bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T22:18:31.755081Z",
     "start_time": "2024-07-05T22:18:27.209462Z"
    }
   },
   "cell_type": "code",
   "source": "pip install --no-cache-dir pillow ",
   "id": "71ebf0c8ded9dd74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in d:\\anaconda3\\lib\\site-packages (9.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Import dependencies ###",
   "id": "7611a29e3fae9949"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T22:26:11.849928Z",
     "start_time": "2024-07-05T22:26:02.500929Z"
    }
   },
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import os.path"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6d28f3e2a2e3d78f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Defining Model ###",
   "id": "251108b72007ee04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:50:59.642533Z",
     "start_time": "2024-07-06T00:50:59.633525Z"
    }
   },
   "cell_type": "code",
   "source": "MODEL_FILE = 'Cats.keras'",
   "id": "b7385c44a219e6c9",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:51:00.875002Z",
     "start_time": "2024-07-06T00:51:00.862005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create model from InceptionV3 and imagenet\n",
    "\n",
    "def create_model(num_hidden, num_classes):\n",
    "    \n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "    \n",
    "    # get the output layer, does an avg pooling of output, \n",
    "    # and feed it to a final Dense Layer that we'll train\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(num_hidden, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create a functional model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    return model"
   ],
   "id": "333a0a47bfc52838",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4. Loading existing model ###",
   "id": "694f5c224920eaca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:51:02.370868Z",
     "start_time": "2024-07-06T00:51:02.359865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loads an existing model, but sets only the last 3 layers (which we added)\n",
    "# to be trainable.\n",
    "\n",
    "def load_existing(model_file):\n",
    "    \n",
    "    model = load_model(model_file)\n",
    "    \n",
    "    # set only the last 3 layers to be trainable\n",
    "    num_layers = len(model.layers)\n",
    "    for layer in model.layers[:num_layers-3]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[num_layers-3:]:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    return model"
   ],
   "id": "8e3cf4867aaa9900",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5. Training the Model ###   ",
   "id": "c92c81db696c69ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:51:03.846904Z",
     "start_time": "2024-07-06T00:51:03.831892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Trains a model, creates new model if not exists\n",
    "def train(model_file, train_path, validation_path, num_hidden=200, num_classes=5, steps=32, num_epochs=20):\n",
    "    if os.path.exists(model_file):\n",
    "        print(\"\\n--- Loading existing model: %s ---\\n\" % model_file)\n",
    "        model = load_existing(model_file)\n",
    "    else:\n",
    "        print(\"\\n--- Creating new model ---\\n\")\n",
    "        model = create_model(num_hidden, num_classes)\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    # create a checkpoint to save model after each epoch\n",
    "    checkpoint = ModelCheckpoint(model_file)\n",
    "    \n",
    "    # creating a image generator for more training data\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, rotation_range=30)\n",
    "    \n",
    "    # now for test data\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, rotation_range=30)\n",
    "    \n",
    "    # Now we tell the generator where to get the images from.\n",
    "    # We also scale the images to 249x249 pixels.\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=(249, 249),\n",
    "        batch_size=5,\n",
    "        class_mode=\"categorical\"\n",
    "    )\n",
    "    \n",
    "    # We do the same for the validation set.\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_path,\n",
    "        target_size=(249, 249),\n",
    "        batch_size=5,\n",
    "        class_mode='categorical'\n",
    "    )\n",
    "    \n",
    "    # Note that pictures must be sorted into different directories\n",
    "    # the directories each must be named after the corresponding category\n",
    "    \n",
    "    # Finally we train the neural network\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch = steps,\n",
    "        epochs = num_epochs,\n",
    "        callbacks = [checkpoint],\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps = 50\n",
    "    )\n",
    "    print(\"\\n --- Top Layers Trained, Begin Fine Tuning ---\\n\")\n",
    "\n",
    "    # After training top layers, unfreeze some of the topmost base layers\n",
    "    # and train together with the top layers\n",
    "    \n",
    "    for layer in model.layers[:249]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[249:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    model.compile(loss='categorical_crossentropy',optimizer=SGD(learning_rate=0.001, momentum=0.9),metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_generator, steps_per_epoch = steps, epochs = num_epochs, callbacks = [checkpoint], validation_data = validation_generator, validation_steps = 50)\n",
    "    "
   ],
   "id": "3485809faf663810",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6. train.py with all above ###",
   "id": "b2b74af0767b602f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:54:44.285051Z",
     "start_time": "2024-07-06T00:51:07.993624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    train(MODEL_FILE, train_path=\"photos\",validation_path=\"photos\",steps=120, num_epochs=5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "id": "ad8242e4387e850d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating new model ---\n",
      "\n",
      "Found 323 images belonging to 5 classes.\n",
      "Found 323 images belonging to 5 classes.\n",
      "Epoch 1/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 189ms/step - accuracy: 0.6158 - loss: 1.7607 - val_accuracy: 0.5920 - val_loss: 1.2393\n",
      "Epoch 2/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 110ms/step - accuracy: 0.7863 - loss: 0.6302 - val_accuracy: 0.8904 - val_loss: 0.3005\n",
      "Epoch 3/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 148ms/step - accuracy: 0.8708 - loss: 0.3122 - val_accuracy: 0.9080 - val_loss: 0.2637\n",
      "Epoch 4/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 118ms/step - accuracy: 0.8633 - loss: 0.3755 - val_accuracy: 0.9452 - val_loss: 0.1932\n",
      "Epoch 5/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 159ms/step - accuracy: 0.8699 - loss: 0.3800 - val_accuracy: 0.9280 - val_loss: 0.1379\n",
      "\n",
      " --- Top Layers Trained, Begin Fine Tuning ---\n",
      "\n",
      "Epoch 1/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 217ms/step - accuracy: 0.7912 - loss: 0.5205 - val_accuracy: 0.8320 - val_loss: 0.5475\n",
      "Epoch 2/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m19s\u001B[0m 157ms/step - accuracy: 0.8583 - loss: 0.3838 - val_accuracy: 0.9863 - val_loss: 0.0383\n",
      "Epoch 3/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 195ms/step - accuracy: 0.8810 - loss: 0.2912 - val_accuracy: 0.9840 - val_loss: 0.0415\n",
      "Epoch 4/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 142ms/step - accuracy: 0.8609 - loss: 0.3777 - val_accuracy: 0.9863 - val_loss: 0.0381\n",
      "Epoch 5/5\n",
      "\u001B[1m120/120\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 192ms/step - accuracy: 0.9159 - loss: 0.1985 - val_accuracy: 0.9840 - val_loss: 0.0351\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7. Predicting with model ###",
   "id": "3219496a2ed57ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:56:31.854522Z",
     "start_time": "2024-07-06T00:56:31.842527Z"
    }
   },
   "cell_type": "code",
   "source": "MODEL_NAME = 'Cats.keras'",
   "id": "3587d3aeb257db6b",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:56:34.272501Z",
     "start_time": "2024-07-06T00:56:34.259131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "# Our samples directory\n",
    "SAMPLE_PATH = './samples'\n",
    "\n",
    "dict = {0: 'Pallas', 1: 'Persian', 2: 'Ragdolls', 3: 'Singapura', 4: 'Sphynx'}\n",
    "\n",
    "# Takes in a loaded model, an image in numpy matrix format,\n",
    "# And a label dictionary\n",
    "def classify(model, image):\n",
    "    result = model.predict(image)\n",
    "    themax = np.argmax(result)\n",
    "    return (dict[themax], result[0][themax], themax)\n",
    "\n",
    "# Load image\n",
    "def load_image(image_fname):\n",
    "    img = Image.open(image_fname)\n",
    "    img = img.resize((249, 249))\n",
    "    imgarray = np.array(img) / 255.0\n",
    "    final = np.expand_dims(imgarray, axis=0)\n",
    "    return final\n"
   ],
   "id": "a49ee530af7a64e6",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-06T00:56:40.553203Z",
     "start_time": "2024-07-06T00:56:35.850666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test main\n",
    "def main():\n",
    "    print(\"\\n ---Loading model from %s ---\\n\"% MODEL_NAME)\n",
    "    model = load_model(MODEL_NAME)\n",
    "    print(\"Done\")\n",
    "\n",
    "    print(\"\\n ---Now classifying files in %s ---\\n\"% SAMPLE_PATH)\n",
    "    \n",
    "    sample_files = listdir(SAMPLE_PATH)\n",
    "\n",
    "    for filename in sample_files:\n",
    "        filename = join(SAMPLE_PATH, filename)\n",
    "        img = load_image(filename)\n",
    "        label, prob, _ = classify(model, img)\n",
    "        \n",
    "        print(\"We think with certainty %3.2f that image %s is %s.\" % (prob, filename, label))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "94513d086dc160ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ---Loading model from Cats.keras ---\n",
      "\n",
      "Done\n",
      "\n",
      " ---Now classifying files in ./samples ---\n",
      "\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "We think with certainty 1.00 that image ./samples\\Pallas cats_115.jpg is Pallas.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Pallas cats_144.jpg is Pallas.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step\n",
      "We think with certainty 0.99 that image ./samples\\Pallas cats_76.jpg is Pallas.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 62ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Persian cats_225.jpg is Ragdolls.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "We think with certainty 0.98 that image ./samples\\Persian cats_44.jpg is Persian.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Persian cats_88.jpg is Persian.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Ragdolls_215.jpg is Ragdolls.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Ragdolls_53.jpg is Ragdolls.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Ragdolls_57.jpg is Ragdolls.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Singapura cats_143.jpg is Singapura.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 63ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Singapura cats_53.jpg is Singapura.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 67ms/step\n",
      "We think with certainty 0.98 that image ./samples\\Singapura cats_96.jpg is Persian.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 65ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Sphynx cats_168.jpg is Sphynx.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Sphynx cats_62.jpg is Sphynx.\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 66ms/step\n",
      "We think with certainty 1.00 that image ./samples\\Sphynx cats_99.jpg is Sphynx.\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
